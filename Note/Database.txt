
NoSQL
  This type of databases typically don't enforce a schema for the data they store, which can make it easier to adapt applications to changing requirements
  However, it still assumes data has a certain struct, it's just a question of whether the schema is explicit(enforced on write) or implicit (handled on read)

Document-based DB
  Stored as a JSON-based data which is very flexible to your datastructures
  If your application has mostly one-to-many relationships (tree-structured data) or no relationships between records, the document model is appropriate
  Document databases target use cases where data comes in self-contained documents and relationships between one document and another are rare.

Graph-based DB
  Stored data like a graph (vertex and edge)
  we use this when you have so many-to-many relationships
  For a graph query: it's really simple to do a recursive query such as few line
  But for a traditional sql query: it's really complicated, maybe upto 30 line or more
  The idea is to pick a data model that is suitable for your application
  Use cases: where anything is potentially related to everything (many-to-many relationships)
  Example:
    MATCH
     (person) -[:BORN_IN]-> () -[:WITHIN*0..]-> (us:Location {name:'United States'}),
     (person) -[:LIVES_IN]-> () -[:WITHIN*0..]-> (eu:Location {name:'Europe'})
    RETURN person.name

CODASYL
  Network Model
  Similar to graph but less efficient

Triple-Stores model
  data is stored in the form of very simple three-part statement: (subject, predicate, object)
  The idea is really similar to graph
    For example, (lucy, age, 33) is like a vertex lucy with properties {"age":33}.
  The reasoning behind this design that you should be able to combine your data with someone else's data.
    And if they attach a different meaning to the word, you won't get conflict
  SPARQL is a query language for triple-stores using the RDF data model
  Example:
    PREFIX : <urn:example:>
    SELECT ?personName WHERE {
     ?person :name ?personName.
     ?person :bornIn / :within* / :name "United States".
     ?person :livesIn / :within* / :name "Europe".
    }

Datalog
  Similar to triple-stores but instead of storing data as (subject, predicate, object), it stores data as predicate(subject, object)
  Example:
    Storing data
      name(namerica, 'North America').
      type(namerica, continent).
      name(usa, 'United States').
      type(usa, country).
      within(usa, namerica).
      name(idaho, 'Idaho').
      type(idaho, state).
      within(idaho, usa).
      name(lucy, 'Lucy').
      born_in(lucy, idaho).
    Query data:
      within_recursive(Location, Name) :- name(Location, Name).  // Rule 1
      within_recursive(Location, Name) :- within(Location, Via), // Rule 2
                                          within_recursive(Via, Name).
      migrated(Name, BornIn, LivingIn) :- name(Person, Name), // Rul 3
                                           born_in(Person, BornLoc),
                                           within_recursive(BornLoc, BornIn), // we are using rule 1 here
                                           lives_in(Person, LivingLoc),
                                           within_recursive(LivingLoc, LivingIn). // rule 2 here
      ?- migrated(Who, 'United States', 'Europe').
      /* Who = 'Lucy'. */

Log-based data
  At the core is a log (also called append-only log)
  so whenever we add/update the data, it appends to the log rather than being modified in place
    => Fast in Write
  However, for the read, it has to scan all the rows of that data in the log and retrieve the last state
    => Which leads to O(n) time complexity and is not suitable for high read application
  And also for the filter range is really not efficient like you need to filter like 'abc%' or date between 1980 and between 1990
    => Lead to O(n) time complexity
  How do we avoid eventually running out of space?
    Break the log into segments of certain size by closing a segment file when it reach a certain size,
    We can perform compaction which means throwing away duplicate keys in the logs, and keeping only the most recent update for each key

SSTable
  Sorted String Table
  In log-based, the data key-value pairs stored in order that they were written to disk
  However in SSTable, these data is sorted by key
  Some advantages:
    + Merge segment is simple and efficient. They use merge sort algorithm to do this
      Even if same key appears in multiple segment, we can just keep the value from most recent segment and discard the value from older segment
    + To find a particular key, you don't need to keep an index. Since the data is sorted, you can easily find the data if you know offset its neighbor beforehand
      Example: said you want to look for "handiwork" but you know the offset of "handbag" and "handsome". you know that you only need to scan between those offset
    + you still need an in-memory index to tell you the offset for some keys, but it can be small since you only need to store few key and start to scan from that
  How it stores data:
    + When a write comes it, add it to an in-memory balanced tree structure (red-black tree) (sometimes called memtable)
    + When memtable get bigger than the threshold - write it out to disk as an SSTable file. This new file become the most recent segment of the database.
        While SSTable is being written out to disk, writes can continue to a new memtable instance
    + From time to time, it runs a merging and compaction process in the background to combine these segment files and to discard overwritten or deleted value
    + What happens if the database crashes which can cause the memtable lost?
      - We can keep a separate log on disk to which every write is immediately append ( like the log-based).
        That log is not in sorted order which is doesn't mater since we use it to restore the memtable after a crash
        And after memtable is written out to disk, we can just clean the log and start to do it again for new data
  How it handle read request:
    + It try to find the key in memtable, then in most-recend on-disk segment and then in the next-older segment
  Disadvantages:
    + very slow when it looks for the key not existed in database because it gonna scan from memtable, most recent segment file, the next-older segment file, etcs
      => Like scanning all the data

OLTP
  Online transaction processing
  Usually use for processing transaction in low latency and high availability


